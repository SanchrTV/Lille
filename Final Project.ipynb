{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ebad33-3c0c-4302-ab94-7072f91d2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from PyDictionary import PyDictionary\n",
    "import yake\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a435b322-d98d-4abd-bf04-cbe7a4814f19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21265fe3-f95d-4e81-a71e-f234dd8539b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c37cd63-1091-4e92-b205-385514e3f39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"/Users/alex/Desktop/ВШЭ/Python/Project/df-corpus.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c179017-2bc9-45f1-92f3-2c07ea030cbe",
   "metadata": {},
   "source": [
    "Ключевые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7e69fa-ecbe-4e78-bf3c-b2fc13b4e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yake_keywords(text):\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 1\n",
    "    deduplication_threshold = 0.9\n",
    "    numOfKeywords = 7\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text.lower())\n",
    "    keywords1 = []\n",
    "    for kw in keywords:\n",
    "        keywords1.append(kw[0])\n",
    "    lemmatized_keywords = ' '.join(keywords1)\n",
    "    doc = nlp(lemmatized_keywords)\n",
    "    lemmatized_keywords = [i.lemma_ for i in doc]\n",
    "    lemmatized_keywords = list(set(lemmatized_keywords))     \n",
    "    return lemmatized_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b006ece-65ff-4a6d-9c7a-f6b4a07d5247",
   "metadata": {},
   "source": [
    "Ключевые слова со словарными значениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "97380982-0cba-4333-84d6-844282c00f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_with_definitions(text):\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "    keybert_keywords = [i[0] for i in keywords]\n",
    "    yake = yake_keywords(text)\n",
    "    all_keywords = keybert_keywords + yake\n",
    "    lemmatized_keywords = ' '.join(all_keywords)\n",
    "    doc = nlp(lemmatized_keywords)\n",
    "    lemmatized_keywords = [i.lemma_ for i in doc]\n",
    "    lemmatized_keywords = list(set(lemmatized_keywords))\n",
    "    dictionary = PyDictionary()\n",
    "    definitions = [dictionary.meaning(i, disable_errors=True) for i in lemmatized_keywords]\n",
    "    words_with_definitions = dict(zip(lemmatized_keywords,definitions))\n",
    "    strings = []\n",
    "    for key,item in words_with_definitions.items():\n",
    "        strings.append(\"{} - {}\".format(key.upper(), item))\n",
    "    result = \"\\n\".join(strings)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4f483-7671-4758-96d6-65ebccca1188",
   "metadata": {},
   "source": [
    "Саммари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fd9cd1fa-df11-4333-9da4-7a4becd2d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(text):\n",
    "    docx = nlp(text)\n",
    "    mytokens = [token.text for token in docx]\n",
    "    word_frequencies = {}\n",
    "    for word in docx:\n",
    "        if word.text not in stopwords:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "    sentence_list = [sentence for sentence in docx.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if len(sent.text.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "    list_of_sentences = list(sentence_scores)\n",
    "    summary = \"\"\n",
    "    for i in list_of_sentences:\n",
    "        summary += str(i)+ \" \" \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea24027-777d-4ea0-bce0-bc5f6ec3e30f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus['keywords with definitions'] = corpus['texts'].apply(keywords_with_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f304ac-1eea-4646-bb81-20d322c3162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['summary'] = corpus['texts'].apply(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a1ffed-518e-415f-a046-b77ea24e2064",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2p/20383txx6fd5wgd4_ttdyt_00000gn/T/ipykernel_50314/987787568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "display(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
